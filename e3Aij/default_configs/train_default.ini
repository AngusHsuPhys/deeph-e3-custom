[basic]

; torch_device            string   Device on which model will be trained (cpu or cuda)
; torch_dtype             string   Data type of floating point numbers used during training (float or double)
; save_dir                string   Directory under which training result will be saved
; additional_folder_name  string   The folder containing train result will be named
;                                  "yyyy-mm-dd_hh-mm-ss_<additional_folder_name>"
; simplified_output       boolean  If set to True, detailed loss of each single target will not be printed to stdout,
;                                  but you can still find them in tensorboard
; seed                    int      Random seed to be used during training
; checkpoint_dir          string   Path pointing to model.pkl or best_model.pkl under the directory where 
;                                  result of some previous training is saved.
;                                  All settings in sections [target], [network] will be overwritten by the config in checkpoint.
;                                  Leave empty to start new training.

torch_device = cpu
torch_dtype = float
save_dir = 
additional_folder_name = 
simplified_output = True
seed = 42
checkpoint_dir = 

[train]

; graph_dir               string   Directory of preprocessed graph data xxxx.pkl

; num_epoch               int      Maximum number of training epochs
; batch_size              int      Batch size
; extra_validation        string   

; learning_rate           float    Initial learning rate
; train_ratio             float    Ratio of structures among all that will be used for training
; val_ratio               float    Ratio of strucutres among all that will be used for validation
; test_ratio              float    (test set not implemented yet)

; ReduceLROnPlateau       string   Will be parsed as a python dict object and passed as keyword arguments
;                                  to ReduceLROnPlateau. Leave empty to disable this scheduler.
;                                  https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html
; min_lr                  float    When learning rate decays lower than min_lr, training will be stopped.
;                                  set to -1 to disable this.

; revert_decay_patience   int      
; revert_decay_rate       float
;                                  Sometimes loss will suddenly go up during training and decreases very slowly.
;                                  When validation loss has been more than 2 times of best loss for more than
;                                  <revert_decay_patience> epochs, the model will be reverted to the heretofore best model
;                                  and learning rate will decay by a factor of <revert_decay_rate>.

graph_dir =

num_epoch = 3000
batch_size = 1
extra_validation = []

learning_rate = 0.002
train_ratio = 0.67
val_ratio = 0.33
test_ratio = 0

ReduceLROnPlateau = dict(factor=0.5, cooldown=40, patience=120, threshold=0.05, verbose=True)
min_lr = 3e-5

revert_decay_patience = 20
revert_decay_rate = 0.8

[target]

; target                  string   Only hamiltonian is supported now
; target_blocks_type      string   choices: (all, diag, off-diag, specify)
;                                  all:       train all matrix blocks of hopping in one model
;                                  diag:      only train diagonal blocks
;                                  off-diag:  only train off-diagonal blocks
;                                  specify:   specify the matrix blocks to be trained by hand
; target_blocks           string   This will only take effect when target_blocks_type=specify.
;                                  See explanations at the end of this config
; selected_element_pairs  string   Train only on hoppings between element pairs specified here. 
;                                  Will have no effect if target_blocks_type=specify.
;                                  example: ['42 42', '16 16'] 
;                                  Under this example, only hoppings between Mo-Mo and S-S will be trained.
; convert_net_out         boolean  Please set to False. Option True is still under development.


target = hamiltonian ; only support hamiltonian now
target_blocks_type = all ; choices: specify, all, off-diag, diag
target_blocks =  ; use this when target_blocks_type = specify. notice that this should be consistent with net_out_irreps
selected_element_pairs = ; example: ['42 42', '16 16'] Train only on hoppings between element pairs specified here. Will have no effect if target_blocks_type=specify.
convert_net_out = False ; option True is under development

[network]

; cutoff_radius            float    Cutoff radius of Gaussian basis for edge-length encoding, in Angstrom
; only_ij                  boolean  Please set to False. Option True is still under development.
; spherical_harmonics_lmax int      Maximum angular momentum quantum number used in spherical harmonics
; irreps_embed             string   Irreps used for node- and edge-embedding, should only contain 0e 
; irreps_mid               string   Irreps of edge and node features in intermediate layers
; num_blocks               string   Number of message passing blocks

cutoff_radius = 7.2
only_ij = False
spherical_harmonics_lmax = 4
irreps_embed = 64x0e
irreps_mid = 64x0e+32x1o+16x2e+8x3o+8x4e
num_blocks = 3

; Below are more advanced settings of the irreps used in the network. 
; Usually these can simply be left blank, we will automatically generate the appropriate settings for you.

irreps_embed_node = 
irreps_edge_init = 
irreps_mid_node = 
irreps_post_node = 
irreps_out_node = 
irreps_mid_edge = 
; The best irreps for below will be automatically generated. 
; Adjusting according to your own will might cause errors.
irreps_post_edge = 
out_irreps = 



; =============================
; 
; Explanation of target_blocks
; 
; For example, the compound MoS2 has two types of elements: Mo(42) and S(16). The orbital types of Mo and S are [0, 0, 0, 1, 1, 2, 2] and [0, 0, 1, 1, 2] respectively (this can be found in orbital_types.dat in processed structure folder). This means, The number of atomic orbitals for Molybdenum is 7, containing three S orbitals, two P orbitals and two D orbitals. Similar for the element Sulphur. 
; 
; Suppse we set target_blocks to
; [{"42 42": [3, 5]}]
; This means, when the net sees a hopping matrix between Mo and Mo, it only takes out the hopping between the orbital of Mo which has index 3 (i.e. the first P orbital) and the orbital of Mo which has index 5 (i.e. the first D orbital). The predicted matrix size is thus (2x1+1)x(2x2+1) = 3x5. Other types of hopping (e.g. Mo-S, S-Mo, S-S) are not trained.
; 
; If the target is set to be
; [{"42 42": [3, 5], "42 16": [3, 4], "16 42": [2, 5], "16 16": [2, 4]}]
; Then 4 types of hopping are trained together. Specifically, these are: hopping from 1st P orbital of Mo to 1st D orbital of Mo, 1st P orbital of Mo to 1st D orbital of S, 1st P orbital of S to 1st D orbital of Mo, 1st P orbital of S to 1st orbital of S. These orbitals will be predicted in the same output channel.
; 
; If the target is set to be
; [{"42 42": [3, 5], "42 16": [3, 4], "16 42": [2, 5], "16 16": [2, 4]}, {"42 16": [3, 2]}]
; In addition to the orbitals described above, the new dict in the list {"42 16": [3, 2]} introduces a new independent channel in the output. This channel predicts the hopping from the 1st P orbital of Mo to the 1st P orbital of S.
; 
; Note that the angular quantum numbers should always be the same for orbitals predicted in the same channel, or error will be thrown out.

